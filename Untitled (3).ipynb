{
 "cells": [
  {
   "cell_type": "raw",
   "id": "ceff48ce-5034-46d8-826e-9864edffbe12",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "       WebScraping Assignment 3\n",
    "\n",
    "\n",
    "Q.1\n",
    "\n",
    "\n",
    "import requst from bs4 import BeautifulSoup\n",
    "def search_product(product):\n",
    "    # Formet the search query search_query = product.replace(' ' , '+') \n",
    "    # Send a GET request to Amazon. in search page url= f'https://www.amazon.in/s?k={search_query}'\n",
    "    responce=request.get(url)\n",
    "    # parse the HTML content using BeautyfulSoup soup=BeautyfulSoup(responce.content,'html.parser')\n",
    "# Find all the product element on the pages products=soup.find_all('div' , {'data- component-type': 's-search-result'})\n",
    "# Interate over the products and exract relevent imformation for product in products:\n",
    "#Extract the product title \n",
    "title=product.find('span',{'class':'a_size-medium'}).text.strip()\n",
    "# Extract theb product rating rating=product.find\n",
    "('span',{'class':'a-icon-alt'}).text scrip()\n",
    "#Print the product impormatin \n",
    "print(f'Title:{title}')\n",
    "print(f'Price: {price}')\n",
    "print(f'Price: {rating}')\n",
    "print('---')\n",
    "\n",
    "#Take user input for the product to search \n",
    "product=input('Enter the product to search: ')\n",
    "\n",
    "#Call the search_product function with the user input search_product(product)\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75779fca-4c6f-46b8-9753-ce92c1f4cedd",
   "metadata": {},
   "outputs": [],
   "source": [
    " Q .3\n",
    "\n",
    "\n",
    "\n",
    "# Set the path to the web driver expecteble\n",
    "drive_path='path_to_chromedriver'\n",
    "\n",
    "# Create a new page instance of the Chrome driver\n",
    "driver=webdriver.Chrome(driver_path)\n",
    "\n",
    "#Open images.google.com driver.get('https://images.google.com')\n",
    "\n",
    "# Finde the search bar element and  enter the keyword \n",
    "    search_bar=driver.find_element(By.NAME,'q')\n",
    "    keyword=['fruits',Cars' 'Machine Learning','Guitar', Cakes']\n",
    "    for keyword in keywords: search_bar.clear()\n",
    "    search_bar.send_keys(keys.(keyword)\n",
    "    search_bar. send_keys(keys.RETURN\n",
    "# Wait for the search result to load WebDriverWait(driver,10).until\n",
    "   (EC.presence_of_element  'rg_i')))\n",
    "#Scrape the image URLs image_elements=driver.find_elements\n",
    "   (By.CLASS_NAME,'rg_i')))image_urls=\n",
    "   [element.get_attribute9'src')for element in image_element]\n",
    "# Print the first 10 image URLs\n",
    "    print(f\"Top 10 images for '{keyword]':\")\n",
    "    forurl in image_urls[:10]:\n",
    "    print(url)\n",
    "# Close the browser driver.quit()\n",
    "\n",
    "                         \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30c71cd4-cf26-45dd-b518-a5236e786e23",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q. 4 \n",
    "\n",
    "\n",
    "\n",
    "import requsstsvfrom bs4 import BeatifulSoup import pandas as pd\n",
    "\n",
    "#Function to scrape smartphone details def scrape_smartphones():url=\n",
    "  \"https://www.flipkart.com/search q=smartphone\"  \n",
    "   #Replace\"smartphone\"with  your desiredsearch query\n",
    "   responce=request.getz(url)soup=\n",
    "BeautifulSoup(responce.content,'httml.parser')\n",
    "\n",
    "                 smartphone=[]\n",
    "\n",
    "           result=\n",
    "soup.find_all('div',{'class':'1AtVbE'})\n",
    "\n",
    "    for result in results: details={}\n",
    "        \n",
    "#Extracting details from each search result \n",
    "  details['Brand Name']=result. find('div',{'class':'_4rR01T'}).text\n",
    "  details['Smartphhone Name ']=result.find('a',{'class':'_2WkVrV'}).text\n",
    "   details['RAM']=\n",
    "result.find('ul',{'class':'_1xgFaf'}). find_all('li')\n",
    "\n",
    "[1].text\n",
    "\n",
    "details['primary Camera']=\n",
    "    result.find('ul',{'class':'_1xgFaf'}). find_all('li\n",
    "[2].text \n",
    "                                                    \n",
    "details['secondary Camera'] result.find\n",
    "    (ul',{;class':'_1xgFaf'}) .find_all('li')\n",
    "                                                    \n",
    "\n",
    " [3].text\n",
    "                                                    \n",
    "   details['Display Size']=result.find \n",
    "      ('ul',{'class':'_1xgFgf'}).find_all('li')\n",
    "\n",
    "[4].text \n",
    "  details['Battery Capacity']=result.find('div',{'class':\n",
    "  '_30jeq3 _1_WHN1'}). text details['product URL']=\"https://www.flipkart.com\"+\n",
    "    result.find('a',{'class':'IRpwTa'})['href']\n",
    "\n",
    "       smartphone.append(details) \n",
    "       return smartphone\n",
    "# Scrape smartphone details\n",
    "  smartphone=\n",
    "  scrape_smartphone()\n",
    "# Scrape dataframe from the scraped details \n",
    "  df+pd.DataFrame(smartphone)\n",
    "# Replacemissing details with\n",
    "   \"_\"\n",
    "  df.fillna(\"_\",inplace=True)\n",
    "\n",
    "# Save dataframe to CSV \n",
    "df.to_csv('smartphone.csv',\n",
    "          index=False)\n",
    "\n",
    " \n",
    "                                                    \n",
    "                                                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eecbcfc-1797-4e81-a258-cf695e72453a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "Q.5\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Wait = WebDriveWait(driver, 10)\n",
    "drive.get(\"https://ww.google.com/maps\")\n",
    "wait.until(EC.element_to_be_clickable((By.ID,\"searchboxinput\"))).send_keys(\"New YOEK\")\n",
    "wait.until(EC.element_to_be_clickable((By.ID.\"searchbox-searchbutton\"))).click()\n",
    " sleep(5) \n",
    " ActionChainas(dirver).move_to_element(driver.find_element_by_xpath(\"//html/body\")).context_click().perform()\n",
    " print(wait.until(EC.visibility_of_element_located((By.CSS_SELECTOR, \"ul[role='menu']>li div div[class*='text']:nth-of-type(1)\"))).text)                                      \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44220f68-c2fb-4a7e-911b-fb0f76ab786d",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q.6\n",
    "\n",
    "\n",
    "\n",
    "  # Set up the WebDriver \n",
    "driver=\n",
    "webdriver.Chrome('path_to_chromedriver')\n",
    "# Open the website\n",
    "driver.get('https://www.digit.in/')\n",
    "#Search for gaming laptops\n",
    "search_bar = \n",
    "driver.find_element_by_id('searchDiv')\n",
    "search_bar.send_keys('gaming laptop')\n",
    "search_bar.submit()\n",
    "\n",
    "# Wait for the search result to load time.sleep(2)\n",
    "\n",
    "#Srape the details laptop_element=\n",
    "driver.find_element_by_class_name('searchPage')\n",
    "laptop_details=[]\n",
    "for laptop in laptop_element:\n",
    "    name=\n",
    "    laptop.find_element_by_class_name('searchProductTitle').text\n",
    "    price=\n",
    "    laptop.find_element_by_class_name('searchPrice').text\n",
    "    specifications=\n",
    "    laptop.find_element_by_class_name('searchSpec').text\n",
    "\n",
    "\n",
    "   laptop_details.append({\n",
    "       'Name':name,\n",
    "       'Price':price\n",
    "       'Specifications':\n",
    "specification\n",
    "   })\n",
    "#Print the scraped detailsfor laptop in laptop_details:\n",
    "  print(laptop)\n",
    "# Close the WebDriver\n",
    "driver.quit()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2885c3cd-a26c-417e-a0f0-7097bf3ab11b",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q.7\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import request\n",
    "from bs4 import BeautifulSoup\n",
    "# Send a GET requet to the Ferbes website\n",
    "url=\n",
    "\"hhtp://www.forbes.com/billionaires/\"\n",
    "responce=requests.get(url)\n",
    "\n",
    "# Find the table containing the billionaire details \n",
    "table=soup.find(\"table\",\n",
    "                classa-=\"table\")\n",
    "# Find all the rows in the table\n",
    "rows=table.find_all(\"tr\")\n",
    "# Iterate over each row and extract the\n",
    " required details for row in rows:\n",
    "\n",
    "# Extract the required details from the columns\n",
    "                 rank=\n",
    "columns[0].text.strip()\n",
    "    name=\n",
    "columns[1].text.strip()\n",
    "    net_worth=\n",
    "columns[2].text.strip()\n",
    "    age=\n",
    "columns[3].text.strip()\n",
    "    citizenship=\n",
    "columns[4].text.strip()\n",
    "    source=\n",
    "columns[5].text.strip()\n",
    "    industry=\n",
    "columns[6].text.strip()\n",
    "\n",
    "#Print  the extracted details\n",
    "  print(\"Rank:\",rank)\n",
    "  print(\"Name:\",name)\n",
    "  print(\"Net Worth:\",\n",
    "net_worth)\n",
    "   print(\"Source:\",source)\n",
    "   print(\"Industry:\", industry)\n",
    "   print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6145ea42-b548-4407-9e1e-0850316fc701",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q.8\n",
    "\n",
    "\n",
    "\n",
    "  from selenium import webdriver import time\n",
    "# Set up the WebDriver\n",
    " driver=\n",
    "webdriver.chrome('path_to_chromedriver')\n",
    "#Replace with the path to your WebDriver executable\n",
    "# open the YouTube video\n",
    "video_url=\n",
    "'https://www.youtube_id' # Replace with the URLof the YouTube videi\n",
    "driver.get(video_url)\n",
    "# Scroll to load comments scroll_PAISE_TIME = 2  # Adjust the pause time as needed\n",
    "scrolls + 10 # Adjust the number of scroll as needed\n",
    "for _ in range(scrolls):\n",
    "  driver.execute_scrop(\"window.scrollTo(0,\n",
    "  document.documenteElement.crollHight);\")\n",
    "\n",
    "time.sleep(scroll_pause_time)\n",
    "\n",
    "# Extract comments,upvotes,and time\n",
    "comments=\n",
    "driver.find_element_by_xpath('//yt-\n",
    " formetted-string[@id=\"content-text\"]')\n",
    " upvotes =\n",
    " driver.find_elements_by_xpath('//a[@class=\"yt-\n",
    " simple-endpoint style-scope\n",
    " yt-formetted-string\"]\")\n",
    "\n",
    " # Store the extracted data extracted_data=[]\n",
    " for comment,upvite,time in zip(comment,upvotes,time):\n",
    "   extracted_data.append({\n",
    "   'comment':comment.text,\n",
    "   'upvote': upvete.text,\n",
    "   'time';time.text\n",
    "   })\n",
    "\n",
    "   # Close the WebDriver driver.quit()\n",
    "\n",
    " # Print the extractes data for data in extracted_data:\n",
    " print(data)\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "103b4ee3-fecb-4df5-83a1-987b7957f881",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q.9\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "improt request from bs4 import BeautifulSoup\n",
    "\n",
    "# Send a GET request to the website\n",
    "url=\n",
    "\"https://www.hostelword.com/hostels/London\"\n",
    "responce = request.get(url)\n",
    "\n",
    "# Create a BeautifulSoup\n",
    "object to parse the HTML Content\n",
    "Soup =\n",
    "BeautifulSoup(responce.content,\n",
    "\"html.parser\")\n",
    "\n",
    "# Find all the hostel containers\n",
    "hostels = Soup.find_all(\"div\",\n",
    "     class_=\"febresult')\n",
    " # Iterate over each hostel container\n",
    "    and extract the required information for hostel  in hostels:\n",
    "# Extract hostel name name = hostel.find(\"h2\",\n",
    "   class_=\"febresult-title\").text.strip()\n",
    "#  Extract totel reviews tote reviews totel_reviews=\n",
    "   hostel.find(\"div\",\n",
    "    class_=\"overall\").text.strip()\n",
    "\n",
    "# Extract privates from price\n",
    "  privets_price=hostel.find(\"div\",\n",
    "     class_=\"price-col\").find(\"div\",class_=\"price\").text.strip()\n",
    "\n",
    "# Extract property description\n",
    "     discription=\n",
    "hostel.find(\"div\",class_=\"description\").text.strip()\n",
    "\n",
    "# Print the extracted information\n",
    "  print(\"Hostel Name :\", name )\n",
    "  print(\"Distance from City Centre:\", distance)\n",
    "  print(\"Ratings:\",rating)\n",
    "  print(\"Totle Reviews:\",totle_reviews)\n",
    "  print(\"Overoll Reviews:\",overoll_reviwes)\n",
    "  print(\"Privetes form Price:\",privetes_price)\n",
    "        print(\"Dorms from Price:\", droms_price\n",
    "   print(\"Facilities:\", facilities)\n",
    "              Description:\",description)\n",
    "print()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
